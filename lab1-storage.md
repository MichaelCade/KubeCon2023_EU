# Kubernetes Storage (Lab 1)
Although you will find full commands below, all YAML manifests are included them in the `Lab1\` folder within the repository.

This lab is based on you completing the pre-reqs and having access to a minikube cluster outlined in the readme.md

I have also added in the Lab1 folder a simple demo where we deploy postgreSQL as a stateless pod, can you guess what happens once you create the database and then delete the pod?

- Ephemeral Volumes
- Projected Volumes
- Persistent Volumes
- Persistent Volume Claims
- Storage Classes
    - Dynamic Provisioning (PVC)

### Task 1 - Volumes

In Kubernetes, all containers are ephemeral and Kubernetes volumes are an abstraction implemented to solve two problems.

1. Loss of files when a container crashes.
2. Sharing files between containers running together in a Pod.

The volumes fall under three major categories,

1. Ephemeral Volumes
2. Projected Volumes
3. Persistent Volumes


# Ephemeral Volumes

Ephemeral volumes follow the Pod's lifetime and get created and deleted along with the Pod, Pods can be stopped and restarted without being limited to where some persistent volume is available.

In the following example we are going to examine `emptyDir` ephemeral storage.

An `emptyDir` volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node.

The example shows two containers with an `emptyDir` shared volume.

First, the pod has a volume called `shared-vol`. This volume is mounted in the WebServer container at `/var/www/html`, because thatâ€™s the directory the web server serves files from.

The same volume is also mounted in the content-generator container, but at /data, because that's where the generator writes the files to.

By mounting this single volume like that, the web server will now serve the content generated by the content generator.

1. Let's create a namespace called `lab1` first with the following command:

```bash
kubectl create ns lab1
```

2. Apply the manifest file example:

```yaml
cat <<'EOF' > web-server.yaml | kubectl apply -f web-server.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: web-server
  name: web-server
  namespace: lab1

spec:
    volumes:
    - name: shared-vol
      emptyDir: {}

    initContainers:
    - name: content-generator
      image: busybox
      volumeMounts:
      - name: shared-vol
        mountPath: /data

      command: [ "/bin/sh", "-c" ]
      args: [ 'echo \"Website initialized successfully!\" > /data/index.html' ]

    containers:
    - image: httpd
      name: httpd
      volumeMounts:
      - name: shared-vol
        mountPath: /var/www/html/
EOF
```

3. Check the Pod status:

```bash
watch kubectl get pod web-server -n lab1
```
4. Use `CTRL + C` to exit `watch` command once the __STATUS__ of Pod reaches __*Running*__

5. Verify the file generated in by the `content-generator` is now accessible from your `httpd` container::

```bash
kubectl exec web-server -n lab1 -c httpd -- cat /var/www/html/index.html
```

The command will return the content of the `index.html` shared file

```bash
Website initialized successfully!
```

# Projected Volumes
A projected volume maps several existing volume sources into the same directory.

Currently, the following types of volume sources can be projected:

- secret
- downwardAPI
- configMap
- serviceAccountToken

In the following example we are going to configure a projected volume for a pod.

1. Run the following commands to create a `secret` and `configmap`

```
kubectl create secret generic mysecret -n lab1 --from-literal=username=supersecret
kubectl create configmap myconfigmap  -n lab1 --from-literal=config=true
```

You can get information about the objects created by running the following commands. The output options `-osjon` or `-oyaml` will display the resources's complete manifest in the specified format.

```bash
# Creates a ConfigMap, with a single key/value pair, config:true
kubectl get secret mysecret -n lab1 -oyaml
# Creates a Secret, with a single key/value pair, password:supersecret
kubectl get configmap myconfigmap -n lab1 -ojson
```
2. Run the following to verify the contents of each object. You should expect to see the values set in the previous step.

```bash
kubectl get configmap myconfigmap -n lab1 -o jsonpath="{.data}"
kubectl get secret mysecret -n lab1 -o jsonpath="{.data.password}" | base64 --decode
```

3. Next, we are going to configure a Pod with these sources:

```yaml
cat <<'EOF' > projected-volume.yaml | kubectl apply -f projected-volume.yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: lab1
  # These are the label values we want to pass down into our container
  labels:
    lab: "1"
    sky: blue
spec:
  containers:
  - name: container-test
    image: busybox
    imagePullPolicy: IfNotPresent
    tty: true
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    # Each entry includes the type of projected volume, the object/key to be projected, and the path to project into the container
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: password
              path: my-group/my-password
      - configMap:
          name: myconfigmap
          items:
            - key: config
              path: my-group/my-config
      - downwardAPI:
          items:
            - fieldRef:
                fieldPath: metadata.labels
              path: my-group/my-labels
EOF
```


4. Check the Pod status:

```bash
watch kubectl get pod volume-test
```
5. Use `CTRL + C` to exit `watch` command once the __STATUS__ of Pod reaches __*Running*__


6. Run the following to verify the data is now available from within your `volume-test` Pod:

```bash
kubectl exec volume-test -n lab1 -- ls /projected-volume/my-group
kubectl exec volume-test -n lab1 -- cat /projected-volume/my-group/my-config
kubectl exec volume-test -n lab1 -- cat /projected-volume/my-group/my-password
kubectl exec volume-test -n lab1 -- cat /projected-volume/my-group/my-labels
```
You should expect the following output:
```bash
my-config
my-labels
my-password
truesupersecretlab="1"
sky="blue"
```
# Persistent Volumes

A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource and have a lifecycle independent of any individual Pod that uses the PV.

A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes.

1. Let's create a `PersistentVolume` of 10Gi, called `myvolume`. With the following properties:

- accessMode of 'ReadWriteOnce'
- mounted on hostPath '/etc/foo'


```yaml
cat<<'EOF' > pv.yaml | kubectl apply -f pv.yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  # No namespace, as PVs are cluster-scope
  name: myvolume
spec:
  # Defines how much total storage is available to be claimed
  capacity:
    storage: 10Gi
  # Defines which access methods are supported:
  accessModes:
    # ReadWriteOnce (RWO) is typical for block storage, where only a single host can write to the volume at once
    - ReadWriteOnce
    # ReadWriteMany (RWX) is typical for network storage, which can support multiple simultaneous writers
    - ReadWriteMany
  # Empty string storageClassName is used for statically provisioned volumes
  storageClassName: ""
  # Defines where on your Node's filesystem to store the data associated with this PV
  hostPath:
    path: /etc/foo
EOF
```

2. Run a kubectl describe to view the status and details of your PV:

```bash
kubectl describe pv myvolume
```
You should expect the __*Status*__ to appear as __*Available*__.


3. Next, create a `PersistentVolumeClaim` called `mypvc` with the following properties:

- a request of 4Gi of `myvolume`
- accessMode of ReadWriteOnce

```yaml
cat<<'EOF' > pvc.yaml | kubectl apply -f pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: mypvc
  # The claim is specific to a namespace
  namespace: lab1
spec:
  accessModes:
    - ReadWriteOnce
  # Specifies which volume should be used
  volumeName: myvolume
  storageClassName: ""
  resources:
    requests:
      storage: 4Gi
EOF
```

4. Run a kubectl describe to view the status and details of your PVC & PV:

```bash
kubectl describe pvc mypvc -n lab1
kubectl describe pv myvolume
```
You should now expect the __*Status*__ of both the PV and PVC to appear as __*Bound*__, with the PVC's Volume referencing myvolume and the PV's Claim referencing `lab1/mypvc`.

5. Next, we are going to create a busybox pod and use the `PersistentVolumeClaim` we created.

```yaml
cat <<'EOF'> busybox-pod-one.yaml | kubectl apply -f busybox-pod-one.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
  namespace: lab1
spec:

  volumes:
  - name:  my-vol # has to match volumeMounts.name
    # References which PVC in your namespace
    persistentVolumeClaim:
      claimName: mypvc

  containers:
  - args:
    - /bin/sh
    - -c
    - sleep 3600
    image: busybox
    imagePullPolicy: IfNotPresent
    name: busybox
    resources: {}

    volumeMounts:
    # References the spec.volumes.name value defined above
    - name: my-vol # has to match volumes.name
      # Defines where in the container filesystem to mount the PV
      mountPath: /etc/bar
EOF
```
Note that the specified mount path in pod's manifest is `etc/bar` while the specified `hostPath` is `etc/foo` in pv's manifest.

6. Check the Pod status:

```bash
watch kubectl get pod busybox -n lab1
```
7. Use `CTRL + C` to exit `watch` command once the __STATUS__ of Pod reaches __*Running*__

8. Validate data persistence by connecting to the pod and copying '/etc/passwd' to '/etc/foo/passwd' then deleting it. Since we are accessing data path in the pod, `etc/bar` directory will be used

```bash
kubectl exec busybox -n lab1 -it -- cp /etc/passwd /etc/bar/passwd
```

9. Confirm contents has been copied to the PV:
```
kubectl exec busybox -n lab1 -- ls /etc/bar
```
10. As hostPath is simply writing into a directory on your Node, you should expect to see the same contents in minikube host:
```bash
minikube ssh -p kubecon23
ls /etc/foo/ # expect to see passwd
```


11. Delete the Pod:

```bash
kubectl delete pod busybox -n lab1 --force --grace-period 0
```

Since are working with persistent volume, data is not lost even though pod is deleted. 'passwd' still exists in directory `etc/foo` of our host machine.

12. Next, we are going to create a second pod which is identical with the one we created and mount it to the same volume and validate the file we copied exists.

```yaml
cat <<'EOF'> busybox-pod-two.yaml | kubectl apply -f busybox-pod-two.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox-two
  name: busybox-two
  namespace: lab1
spec:

  volumes:
  - name:  my-vol # has to match volumeMounts.name
    persistentVolumeClaim:
      claimName: mypvc

  containers:
  - args:
    - /bin/sh
    - -c
    - sleep 3600
    image: busybox
    imagePullPolicy: IfNotPresent
    name: busybox
    resources: {}

    volumeMounts:
    - name: my-vol # has to match volumes.name
      # Different mount point than the previous Pod
      mountPath: /etc/bar2
EOF
```

13. Once the `busybox-two` Pod is __*Running*__, verify the persisted data is still available:

```bash
kubectl exec busybox-two -n lab1 -- ls /etc/bar2 # will show 'passwd'
```


# Storage Classes

A [StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/) provides a way for administrators to describe the classes, sometimes called "profiles," of storage available on a cluster. Different classes might map to quality-of-service levels, have different data retention settings, or could be used to implement limits and quotas across different teams. Kubernetes itself is not opinionated about what classes represent.

In this example, we'll see how using storage classes allows you to automate the provisioning of your PV - this is referred to as dynamic provisioning.

1. Check the current storage classes created in the cluster:

```bash
kubectl get storageclasses
```
You should observe that each defines a *Provisioner* - this refers to either a built-in, aka "in-tree", Kubernetes storage driver or a 3rd party Container Storage Interface (CSI) driver. The Provisioner is the interface to the underlying physical storage that orchestrates actions such as provisioning, deleting, or snapshotting volumes.

You should also observe that the `standard` Storage Class is shown as (default). The default is specified by adding the `storageclass.kubernetes.io/is-default-class=true` annotation to a StorageClass. For any PVCs that don't explicitly include a Storage Class in their spec, the default will be used.

  > ðŸš© __*WARNING*__:
  >
  > You should only have a single default Storage Class configured on a cluster. Having multiple can result in errors when attempting to provision PVCs!

In addition to *Provisioner*, a StorageClass object also describes other [properties](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource) such as *Parameter*, *Reclaim Policy*, *Volume Binding Mode*, etc... that describes what kind of volume will be created via this class, as well as how it will be bound and deleted.

2. We are going to create a PersistentVolumeClaim  called 'storageclassmypvc' using the newly created storageClass `standard`, with a request of 4Gi and an accessMode of ReadWriteOnce.

```yaml
cat<<'EOF' > sc-pvc.yaml | kubectl apply -f sc-pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: storageclass-mypvc
  namespace: lab1
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 4Gi
EOF
```

3. Check the status the new PVC `storageclass-mypvc`:

```bash
kubectl describe pvc storageclass-mypvc -n lab1
```
You should observe that the __*STATUS*__ is __*Bound*__ already, because the VolumeBindingMode for `standard` is `Immediate`

4. List all PVs on the cluster:
```bash
kubectl get pv
```
You should expect to see a new PV with a generated name whose __*CLAIM*__ corresponds to the PVC `lab1/storageclass-mypvc` from the previous step.

5. Let's create a Pod that consumes the claim we created.

```yaml
cat<<'EOF'> busybox-sc.yaml | kubectl apply -f busybox-sc.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: busybox-sc
  name: busybox-sc
  namespace: lab1
spec:

  volumes:
  - name:  my-vol # has to match volumeMounts.name
    persistentVolumeClaim:
      claimName: storageclass-mypvc

  containers:
  - args:
    - /bin/sh
    - -c
    - sleep 3600
    image: busybox
    name: busybox-one

    volumeMounts:
    - name: my-vol # has to match volumes.name
      mountPath: /etc/foo
EOF
```
6. Let's also find the provisioned host path in PV's spec:
```bash
PV=$(kubectl get pv -o jsonpath='{.items[?(@.spec.claimRef.name=="storageclass-mypvc")].metadata.name}')
kubectl describe pv $PV
```
Which should results in:
```yaml
Name:            <generated-name>
Labels:          <none>
Annotations:     hostPathProvisionerIdentity: <provisionser-id>
                 pv.kubernetes.io/provisioned-by: k8s.io/minikube-hostpath
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Bound
Claim:           lab1/storageclass-mypvc
Reclaim Policy:  Delete
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        4Gi
Node Affinity:   <none>
Message:
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/hostpath-provisioner/lab1/storageclass-mypvc
    HostPathType:
Events:            <none>
```

The __*Source*__ > __*Path*__ corresponds to the directory on the host where data is stored. In a production environment this would be the relevant path for your external storage

7. Observe the provisioned host path:
```bash
minikube ssh -p KubeCon23
ls -l /tmp/hostpath-provisioner/lab1/
```
You should see `drwxrwxrwx`, aka `777` permission for folder `storageclass-mypvc`. This will allow a container to write to the directory regardless of which user context it uses.
